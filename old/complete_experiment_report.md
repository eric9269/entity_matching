# 實體匹配模型實驗報告

## 實驗概述

本報告比較三種不同配置的實體匹配模型在中文商品名稱匹配任務上的表現，重點分析負樣本生成策略和預訓練模型對效果的影響。

## 實驗設計

### 實驗任務
- **目標**：判斷兩個商品名稱是否指向同一商品實體
- **數據**：中文電商平台商品名稱配對
- **評估指標**：PR-AUC、Accuracy、Precision、Recall、F1-score

### 實驗配置對比

| 實驗 | 預訓練模型 | 負樣本策略 | 輸出路徑 |
|------|-----------|-----------|----------|
| **第一次** | shibing624/text2vec-base-chinese | 隨機負樣本 | models/bi_encoder_cosine |
| **第二次** | shibing624/text2vec-base-chinese | BM25 困難負樣本 | models/bm25_bi_encoder_cosine |
| **第三次** | BAAI/bge-large-zh-v1.5 | BM25 困難負樣本 | models/bm25_bge-large-zh-v1.5 |

## 詳細實驗配置

### 共同設定
- **模型架構**：Sentence-BERT (雙編碼器)
- **損失函數**：CosineSimilarityLoss
- **訓練輪數**：3 epochs
- **批次大小**：64
- **數據分割**：80% 訓練，20% 測試，訓練集再分 85% 訓練 15% 驗證
- **閾值優化**：在驗證集上以 F1-score 尋找最佳閾值

### 負樣本生成策略差異

#### 隨機負樣本 (第一次實驗)
```python
# 同平台隨機配對，避開真實 group_id
for pos_sample in positive_samples:
    platform = pos_sample.platform
    candidates = root_data[root_data.Platform == platform]
    candidates = candidates[candidates.RootSku != pos_sample.group_id]
    
    # 隨機選取 3 個負樣本
    random_samples = candidates.sample(n=3)
```

**特點**：
- 負樣本相對簡單，容易區分
- 模型訓練時容易過度自信
- 分數分布偏高

#### BM25 困難負樣本 (第二次、第三次實驗)
```python
from rank_bm25 import BM25Okapi

# 使用 BM25 挑選最相似的負樣本
for pos_sample in positive_samples:
    # 建立 BM25 索引
    tokenized_corpus = [title.split() for title in candidate_titles]
    bm25 = BM25Okapi(tokenized_corpus)
    
    # 計算相似度並挑選前 3 名
    query = pos_sample.e1.split()
    scores = bm25.get_scores(query)
    top_indices = np.argsort(scores)[::-1][:3]
```

**特點**：
- 負樣本在語義上更接近正樣本
- 更具挑戰性，提升模型辨識能力
- 模擬真實應用場景

### 預訓練模型差異

#### shibing624/text2vec-base-chinese
- **架構**：基於 RoBERTa/BERT-base
- **訓練數據**：中文通用語料
- **特點**：社群廣泛使用，穩定可靠
- **模型大小**：~400MB

#### BAAI/bge-large-zh-v1.5
- **架構**：BERT-large
- **訓練數據**：大規模中文檢索語料
- **特點**：2024年最新技術，專為檢索匹配優化
- **模型大小**：~1.2GB

## 實驗結果

### 量化指標對比

| 指標 | 第一次實驗<br/>(隨機負樣本) | 第二次實驗<br/>(BM25 困難負樣本) | 第三次實驗<br/>(BGE + BM25) |
|------|---------------------------|------------------------------|---------------------------|
| **預訓練模型** | shibing624/text2vec-base-chinese | shibing624/text2vec-base-chinese | BAAI/bge-large-zh-v1.5 |
| **Best Threshold** | 0.45 | 0.575 | 0.65 |
| **PR-AUC** | 0.9662 | 0.5871 | 0.5966 |
| **Accuracy** | 0.978 | 0.881 | 0.892 |
| **Precision (類別0)** | 0.996 | 0.958 | 0.968 |
| **Recall (類別0)** | 0.975 | 0.880 | 0.885 |
| **Precision (類別1)** | 0.929 | 0.710 | 0.726 |
| **Recall (類別1)** | 0.988 | 0.884 | 0.913 |
| **F1-score (macro avg)** | 0.972 | 0.852 | 0.867 |

### 實際推論測試對比

我們設計了 8 組測試案例，涵蓋不同類型的商品配對：

| # | 測試案例 | 預期結果 | 第一次實驗 | 第二次實驗 | 第三次實驗 |
|---|----------|----------|-----------|-----------|-----------|
| 1 | 華碩 ROG Phone 8 16G/256G 黑<br/>↔ 華碩 Zenfone 10 8G/128G 白<br/>*(品牌相同，商品不同)* | no match | ❌ match (0.7819) | ✅ no match (0.4042) | ✅ no match (0.2432) |
| 2 | Apple iPhone 15 Pro 256GB Titanium Gray<br/>↔ Apple iPad Pro 11吋 256GB 銀<br/>*(品牌相同，商品類型不同)* | no match | ❌ match (0.5499) | ✅ no match (0.4339) | ✅ no match (0.2660) |
| 3 | 三星 Galaxy S24 Ultra 12G/512G 黑<br/>↔ Apple iPhone 15 Pro 256GB Titanium Gray<br/>*(品牌不同，商品類型相同)* | no match | ✅ no match (0.4276) | ✅ no match (0.2221) | ✅ no match (0.1586) |
| 4 | 小米 14 8G/256G 白<br/>↔ OPPO Find X7 16GB 512GB Blue<br/>*(品牌不同，商品類型相同)* | no match | ❌ match (0.5586) | ✅ no match (0.2406) | ✅ no match (0.2594) |
| 5 | Sony Xperia 1 V 12G/256G 綠<br/>↔ Sony Xperia 1 V 16G/512G 黑<br/>*(品牌相同，型號相同，規格不同)* | no match | ❌ match (0.7110) | ✅ no match (0.4254) | ✅ no match (0.5252) |
| 6 | 華為 Mate 60 Pro 12G/512G 灰<br/>↔ Apple iPad Pro 11吋 256GB 銀<br/>*(完全不同)* | no match | ❌ match (0.5273) | ✅ no match (0.5632) | ✅ no match (0.3479) |
| 7 | ASUS ROG Phone 8 16G/256G 黑<br/>↔ ASUS ROG Phone 8 16GB 256GB Black<br/>*(應該匹配的正確案例)* | match | ✅ match (0.9446) | ✅ match (0.8588) | ✅ match (0.9017) |
| 8 | ASUS ROG Phone 8 16G/256G 黑<br/>↔ Apple iPhone 15 Pro 256GB Titanium Gray<br/>*(完全不同)* | no match | ❌ match (0.4667) | ✅ no match (0.2243) | ✅ no match (0.2463) |

### 推論結果統計

| 實驗配置 | 正確預測 | 錯誤預測 | 正確率 | 主要問題 |
|----------|----------|----------|--------|----------|
| **第一次**<br/>(隨機負樣本) | 2/8 | 6/8 | **25%** | 分數偏高，容易誤判為 match |
| **第二次**<br/>(BM25 困難負樣本) | 8/8 | 0/8 | **100%** | 分數分布合理，辨識準確 |
| **第三次**<br/>(BGE + BM25) | 8/8 | 0/8 | **100%** | 分數更低更保守，辨識精確 |

## BM25 負樣本生成原理

### BM25 算法簡介
BM25 (Best Matching 25) 是一種經典的信息檢索算法，用於計算查詢詞與文檔的相關性分數。

**核心公式**：
```
BM25(D,Q) = Σ IDF(qi) × (f(qi,D) × (k1+1)) / (f(qi,D) + k1 × (1-b+b×|D|/avgdl))
```

### 在負樣本生成中的應用

#### 流程說明
1. **候選池準備**：從同平台 root 商品中排除真實匹配
2. **文本分詞**：對查詢和候選商品名稱進行分詞
3. **BM25 索引**：建立語料庫的 BM25 索引
4. **相似度計算**：計算查詢與所有候選的 BM25 分數
5. **困難負樣本選取**：挑選分數最高的前 K 個作為負樣本

#### 實際範例
```python
# 正樣本: "華碩 ROG Phone 8 16G/256G 黑"
# BM25 會優先選取:
# 1. "華碩 ROG Phone 7 12G/256G 黑" (分數最高 - 品牌、產品線相同)
# 2. "華碩 Zenfone 10 8G/128G 白"     (品牌相同，部分規格相同)
# 3. "華碩 Vivobook 15 筆電 灰"       (品牌相同但類型不同)

# 而不會選到:
# "Apple iPhone 15 Pro 256GB"        (品牌完全不同，BM25 分數低)
```

## 關鍵發現與分析

### 1. 負樣本策略的影響

#### 隨機負樣本的問題
- **量化指標虛高**：PR-AUC 達到 0.9662，看似效果優異
- **實際應用失效**：在真實測試中正確率只有 25%
- **模型過度自信**：分數分布偏高，threshold 0.45 導致大量誤判
- **訓練簡單**：負樣本太容易區分，模型沒學到細微差異

#### BM25 困難負樣本的優勢
- **量化指標合理**：雖然 PR-AUC 降至 0.5871，但更貼近實際
- **實際應用優秀**：在真實測試中達到 100% 正確率
- **分數分布合理**：只有真正相似的商品才獲得高分
- **訓練困難**：負樣本具挑戰性，提升模型辨識能力

### 2. 量化指標 vs 實用性的矛盾

這個實驗揭示了機器學習中的一個重要問題：
- **高量化指標 ≠ 好的實用性**
- **需要貼近實際應用的測試**
- **困難樣本比簡單樣本更有價值**

### 3. 真實應用場景考量

在實際商品匹配應用中：
- **候選集規模大**：如 100×100 = 10,000 個可能配對
- **正樣本稀少**：真實匹配通常 ≤ 100 對
- **負樣本佔優**：≥ 99% 都是 "no match"
- **Recall 更重要**：不能漏掉真實匹配對

## 第三次實驗結果分析

### BAAI/bge-large-zh-v1.5 的實際表現

第三次實驗使用了最新的 BGE-large 模型結合 BM25 困難負樣本，結果符合預期：

#### 量化指標改進
- **PR-AUC 提升**：從 0.5871 提升至 0.5966（+1.6%）
- **Accuracy 提升**：從 0.881 提升至 0.892（+1.2%）
- **Recall (類別1) 顯著提升**：從 0.884 提升至 0.913（+3.3%）
- **Threshold 更保守**：從 0.575 提升至 0.65，模型更謹慎

#### 推論測試表現
- **維持 100% 正確率**：與第二次實驗相同，所有 8 個測試案例都正確
- **分數分布更保守**：相似度分數普遍更低，更不容易誤判
- **語義理解更精確**：特別是在品牌相同但商品不同的案例上

#### 關鍵改進點
1. **更嚴格的相似度判斷**：
   - 案例 1：0.4042 → 0.2432（品牌相同商品不同）
   - 案例 2：0.4339 → 0.2660（品牌相同類型不同）
   - 案例 3：0.2221 → 0.1586（完全不同品牌）

2. **正確匹配的穩定性**：
   - 案例 7：0.8588 → 0.9017（真正匹配的案例分數更高）

3. **更好的 Recall**：
   - 類別 1 的 Recall 從 0.884 提升至 0.913
   - 在大規模應用中能更好地召回真實匹配

## 第三次實驗預期 ✅ 已驗證

### ✅ 預期達成的改進
1. **更準確的語義理解**：✅ 分數分布更合理，辨識更精確
2. **更穩定的分數分布**：✅ threshold 提升至 0.65，模型更保守
3. **更高的實際應用效果**：✅ 維持 100% 正確率，且分數更有信心

### 🔍 意外發現
1. **BGE 模型更保守**：相似度分數普遍較低，但判斷更準確
2. **Recall 提升顯著**：對真實匹配的召回能力明顯改善
3. **穩健性更佳**：在各種類型的測試案例上都表現穩定

## 實驗結論（完整版）

基於三次實驗的完整結果：

### 1. 負樣本策略是關鍵因素
- **BM25 困難負樣本** 遠優於 **隨機負樣本**
- 第二次和第三次實驗都達到 100% 推論正確率
- 量化指標雖然降低，但實用性大幅提升

### 2. 預訓練模型的改進效果
- **BGE-large-zh-v1.5** 相比 **text2vec-base-chinese** 有明顯提升
- PR-AUC: 0.5871 → 0.5966 (+1.6%)
- Recall (類別1): 0.884 → 0.913 (+3.3%)
- 分數分布更保守，降低誤判風險

### 3. 量化指標 vs 實用性的最終結論
- **第一次實驗**：高指標 (PR-AUC: 0.9662) 但低實用性 (25% 正確率)
- **第二、三次實驗**：中等指標但高實用性 (100% 正確率)
- 證明了實際測試比量化指標更重要

### 4. 最佳實踐建議

#### 對於中文商品匹配任務：
1. **首選配置**：BAAI/bge-large-zh-v1.5 + BM25 困難負樣本
2. **次選配置**：shibing624/text2vec-base-chinese + BM25 困難負樣本
3. **避免配置**：任何模型 + 隨機負樣本

#### 對於不同應用場景：
1. **高精度場景**：使用 BGE-large + BM25，threshold 0.65
2. **高召回場景**：可適當降低 threshold 至 0.5-0.6
3. **資源受限場景**：使用 base 版本模型，但必須配合 BM25

### 5. 實際部署建議

基於實驗結果，推薦的生產環境配置：
- **模型**：BAAI/bge-large-zh-v1.5
- **訓練策略**：BM25 困難負樣本（負樣本比例 1:3）
- **閾值**：0.65（可根據業務需求微調）
- **硬體要求**：RTX 3090 或以上級別 GPU

---

**實驗完成狀態**：✅ 全部完成  
**最終更新**：2025年9月11日

## 後續工作

### 1. 完成第三次實驗 ✅
- ✅ 訓練 BAAI/bge-large-zh-v1.5 + BM25 困難負樣本
- ✅ 獲得完整的三次實驗對比數據
- ✅ 驗證 BGE 模型的改進效果

### 2. 進階實驗設計（建議）
- 測試更多預訓練模型（如 uer/sbert-base-chinese-nli）
- 嘗試不同的負樣本比例（1:1, 1:3, 1:5）
- 實驗不同的損失函數（Triplet Loss, Multiple Negatives Ranking Loss）

### 3. 大規模評估
- 設計 10,000 對規模的測試集
- 模擬真實應用場景的性能測試
- 分析不同商品類型的匹配效果

---

**報告狀態**：待第三次實驗完成後更新  
**最後更新**：2025年9月11日
